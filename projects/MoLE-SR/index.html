<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Mixture of LoRA Experts for Universal and Efficient Image Super-Resolution.">
  <meta name="keywords" content="Image super-resolution, Low-rank adaptation, Mixture of experts, Few-shot domain adaptation, Parameter-efficient fine-tuning.">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MoLE-SR: Mixture of LoRA Experts for Universal and Efficient Image Super-Resolution</title>

  <!-- 字体 -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <!-- 样式 -->
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <!-- 网站图标 -->
  <link rel="icon" href="./static/images/favicon.svg">

  <!-- JS 库 -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MoLE-SR: Mixture of LoRA Experts for Universal and Efficient Image Super-Resolution</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Yao Zhang<sup>1</sup>,</span>
            <span class="author-block">
              Zhengxue Cheng<sup>1</sup>,</span>
            <span class="author-block">
              Xinning Chai<sup>1</sup>,
            </span>
            <span class="author-block">
              Li Song<sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Shanghai Jiao Tong University</span>
          </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Interactive Visualization (JuxtaposeJS Slider) -->
<section class="section">
  <div class="container is-max-desktop has-text-centered">
    <h2 class="title is-3">Interactive Super-Resolution Comparison</h2>

    <!-- JuxtaposeJS container -->
    <div id="juxtapose-slider" style="max-width:500px; margin:0 auto;"></div>

    <!-- Thumbnail selection bar -->
    <div class="thumb-bar" style="display:flex; justify-content:center; margin-top:15px; gap:8px;">
      <img src="./static/thumbs/case1.png" data-lq="./static/LR/case1.png" data-hr="./static/HR/case1.png" class="active" style="width:90px; height:70px; cursor:pointer; border:2px solid #3273dc;">
      <img src="./static/thumbs/case2.png" data-lq="./static/LR/case2.png" data-hr="./static/HR/case2.png" style="width:90px; height:70px; cursor:pointer;">
      <img src="./static/thumbs/case3.png" data-lq="./static/LR/case3.png" data-hr="./static/HR/case3.png" style="width:90px; height:70px; cursor:pointer;">
      <img src="./static/thumbs/case4.png" data-lq="./static/LR/case4.png" data-hr="./static/HR/case4.png" style="width:90px; height:70px; cursor:pointer;">
      <img src="./static/thumbs/case5.png" data-lq="./static/LR/case5.png" data-hr="./static/HR/case5.png" style="width:90px; height:70px; cursor:pointer;">
    </div>
  </div>
</section>

<!-- JuxtaposeJS resources -->
<link rel="stylesheet" href="https://cdn.knightlab.com/libs/juxtapose/latest/css/juxtapose.css">
<script src="https://cdn.knightlab.com/libs/juxtapose/latest/js/juxtapose.min.js"></script>

<script>
function initSlider(lqPath, hrPath) {
  // 清空之前的滑块
  document.getElementById('juxtapose-slider').innerHTML = '';

  // 创建新的滑块
  new juxtapose.JXSlider('#juxtapose-slider',
    [
      { src: lqPath, label: 'Zoomed LQ images', credit: '' },
      { src: hrPath, label: 'Ours results', credit: '' }
    ],
    {
      animate: true,
      showLabels: true,
      showCredits: false,
      startingPosition: "50%",
      makeResponsive: true
    }
  );
}

// 初始化第一个案例
document.addEventListener('DOMContentLoaded', function() {
  const firstThumb = document.querySelector('.thumb-bar img.active');
  initSlider(firstThumb.dataset.lq, firstThumb.dataset.hr);

  // 点击切换图集
  document.querySelectorAll('.thumb-bar img').forEach(img => {
    img.addEventListener('click', function() {
      document.querySelectorAll('.thumb-bar img').forEach(t => t.classList.remove('active'));
      this.classList.add('active');
      this.style.borderColor = '#3273dc';
      initSlider(this.dataset.lq, this.dataset.hr);
    });
  });
});
</script>



  
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Most image super-resolution (SR) models trained on natural images achieve strong performance on benchmark datasets. However, their performance degrades 
            when applied to unseen domains whose semantic content or degradation types differ substantially. A common remedy is class-driven adaptation, which fine-tunes 
            model per manually defined class (either a semantic category or a degradation type), but this approach is subjective, data-hungry and often causes 
            catastrophic forgetting of previously learned domains.
            To address these challenges, we propose \emph{MoLE-SR} (Mixture of LoRA Experts for Super-Resolution), a representation-driven universal SR framework that 
            achieves efficient few-shot adaptation to unseen (out-of-training) domains while preserving performance on seen (in-training) domains. MoLE-SR adopts a 
            decoupled two-stage learning strategy. In Stage~1, a shared SR backbone is combined with multiple lightweight LoRA experts, each trained separately on 
            representative in-training domains to learn domain-specific average representation, yielding notable performance gains with only a small fraction of 
            parameters updated. In Stage~2, given only a few samples from an out-of-training domain, all experts remain fixed while a compact layer-wise router is 
            optimized to dynamically select and compose experts under the guidance of samples' representations, enabling effective adaptation with negligible 
            additional parameter overhead.
            Extensive experiments across diverse SR architectures and a wide range of out-of-training domains demonstrate that MoLE-SR preserves in-training-domain 
            performance without degradation, substantially improves transferability in few-shot settings with minimal extra data and computation.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

</section>

  <!-- Method -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Method</h2>
    
    <!-- Overview Image -->
    <figure class="image is-3by2">
      <img src="./static/images/overview.png" alt="Overview of the method">
    </figure>

    <!-- Description -->
    <div class="content has-text-justified">
      <p>
        Our method consists of three main steps: data acquisition, deformation field estimation, and canonical neural radiance field reconstruction. 
        In the first step, we capture multi-view images or videos using mobile devices. Next, we estimate a continuous volumetric deformation field that maps 
        observed points to a canonical space. Finally, we use a NeRF-based rendering pipeline to generate photorealistic views from arbitrary viewpoints.
      </p>
      <p>
        The overview diagram above illustrates how raw input frames are transformed into a deformable NeRF representation, enabling realistic rendering of 
        dynamic objects.
      </p>
    </div>
  </div>
</section>
<!--/ Method -->


<!-- Visual Results -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Visual Results</h2>

    <!-- In-domain Test Sets -->
    <div class="content has-text-centered">
      <h3 class="title is-4">In-domain Test Datasets</h3>
      <figure class="image">
        <img src="./static/images/in_domain.png" alt="In-domain test set visualization">
      </figure>
      <p>Visualization of in-domain samples.</p>
    </div>

    <!-- Out-of-domain Test Sets -->
    <div class="content has-text-centered">
      <h3 class="title is-4">Out-of-domain Test Datasets</h3>
      <figure class="image">
        <img src="./static/images/out_domain.png" alt="Out-of-domain test set visualization">
      </figure>
      <p>Visualization of out-of-domain samples.</p>
    </div>

  </div>
</section>




<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website adapted from the following <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
